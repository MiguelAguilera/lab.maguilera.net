---
title: "Research"
layout: default
excerpt: "Aguilera Lab -- Research"
sitemap: false
permalink: /research/
---

# Research

### Statistical Physics of Transformer Neural networks

<img class="card-img" src="https://lab.maguilera.net/images/research/attractor_attention.png" width="260" style="padding-left: 20px" alt="mean-field dynamics attention neural network" align="right" />

We exploit methods from nonequilibrium statistical physics and the disordered physics of associative memories to describe the macroscopic variables that guide the behavior of attention mechanisms in transformer models. By investigating the relationship between attention mechanisms and associative memories (aka asymmetric Hopfield networks), we aim to uncover the complex dynamics of such networks. Using path integral methods over generating functionals, we derive analytical approximations for large self-attention neural networks, revealing phenomena such as nonequilibrium phase transitions and chaotic bifurcations. This approach enhances the interpretability of transformer models and aims to reduce computational training costs.

1. Poc-LÃ³pez A & **Aguilera** M (2024). [Dynamical Mean-Field Theory of Self-Attention Neural Networks](https://arxiv.org/abs/2406.07247). _arxiv preprint_.\
[![DOI](https://img.shields.io/badge/DOI-10.48550/arXiv.2406.07247--y-lightgreen.svg)](https://doi.org/10.48550/arXiv.2406.07247)

